---
title: "Final Project"
subtitle: "MATH 40028/50028: Statistical Learning"
date: March 13, 2024
output: pdf_document

fontfamily: mathpazo
fontsize: 11pt
header-includes:
   - \linespread{1.05}
urlcolor: blue
---

__ACADEMIC INTEGRITY: Every student should complete the project by their own. A project report having high degree of similarity with work by any other student, or with any other document (e.g., found online) is considered plagiarism, and will not be accepted. The minimal consequence is that the student will receive the project score of 0, and the best possible overall course grade will be D. Additional consequences are described at http://www.kent.edu/policyreg/administrative-policy-regarding-student-cheating-and-plagiarism and will be strictly enforced.__

## Instruction

__Goal:__ The goal of the final project is to apply the statistical learning methods discussed in this course to perform predictive analysis of real-life data. You will need to identify prediction problem(s), carry out necessary exploratory data analysis, perform predictive analysis using statistical learning methods, assess the performance, and communicate the results in a report. 

__Report:__ Use this Rmd file as a template. Edit the file by adding your project title in the YAML, and including necessary information in the four sections: (1) Introduction, (2) Statistical learning strategies and methods, (3) Predictive analysis and results, and (4) Conclusion. 

__Submission:__ Please submit your project report as a PDF file (8-10 pages, flexible) to Canvas by __11:59 p.m. on May 5, 2024__. The PDF file should be generated by “knitting” the Rmd file. You may choose to first generate an HTML file (by changing the output format in the YAML to `output: html_document`) and then convert it to PDF. Word documents, however, cannot be used as an intermediate file (and of course, the submitted file). __20 points will be deducted if the submitted files are in wrong format.__ 

__Grade:__ The project will be graded based on your ability to (1) recognize and define prediction problems, (2) identify potentially useful statistical learning methods, (3) perform the predictive analysis and assess the performance, (4) document the analysis procedure (with R code) and clearly present the results, and (5) draw valid conclusions supported by the analysis.

__Datasets:__ You may consider (but are not restricted) to use the following packages/datasets. 

* [`ISLR2`](https://cran.rstudio.com/web/packages/ISLR2/ISLR2.pdf): datasets used in the _Introduction to Statistical Learning_ textbook
* [`dslabs`](https://cran.r-project.org/web/packages/dslabs/dslabs.pdf)
* [UCI Machine Learning Repository](https://archive.ics.uci.edu/)

\pagebreak

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
# Load necessary libraries
library(ISLR)
library(ggplot2)
library(caret)
```


## Introduction [15 points]

* Describe the dataset. What is the dataset about? 
* If possible, comment on the target population, sampling strategies, potential bias, etc. 
* Identify and define prediction problem(s).
* Discuss how to split the data into training and test sets among other plans for the use of data.

```{r, echo=TRUE}

data("Carseats")
dataset = Carseats
head(dataset)
```

```{r, echo=TRUE}
str(dataset)
```

```{r, echo=TRUE}
summary(dataset)
```

__Describe the dataset. What is the dataset about?__ 

The ISLR package's "Carseats" dataset comprises 400 entries across 11 variables, offering insights into car seat sales factors. Key attributes include "Sales" (unit sales), "CompPrice" (competitor prices), "Income" (community income), and "Advertising" (local ad budgets). "Population" indicates region size, and "Price" represents car seat prices. "ShelveLoc" categorizes shelf quality, "Age" denotes population age, and "Education" reflects education levels. Binary variables "Urban" and "US" specify urban/rural and US/non-US locations. This dataset facilitates analysis of pricing, advertising, location, demographics, and geography's impact on sales. Such insights aid marketing strategies, pricing decisions, and market expansion plans for car seat manufacturers and retailers.x

__If possible, comment on the target population, sampling strategies, potential bias, etc.__
 
The "Carseats" dataset likely targets consumers where car seats are sold, aiming for diversity across regions, urban/rural areas, and income levels. Yet, biases may arise from excluding certain demographics or regions. Over representation or under representation of stores/regions could skew results, favoring affluent urban areas, for example. Selection bias may occur if only certain stores participate or based on specific characteristics. This could omit challenges faced by lower-sales stores. Awareness of these biases is vital for accurate interpretation and generalization of findings. Sampling strategies, while diverse, must address these biases to ensure a comprehensive representation of consumer behaviors across varied demographics and locations.


__Potential biases in the dataset could arise due to several factors:__

Potential biases in the "Carseats" dataset may arise due to various factors. Firstly, there could be sampling bias if the data predominantly represents certain demographic or geographic regions, leading to skewed insights. Moreover, biases may stem from the representation of only certain income levels or urban areas, neglecting rural or lower-income populations. Additionally, the dataset's reliance on sales data may introduce bias towards popular or well-advertised car seat models, overlooking niche or less-promoted products. Biases might also emerge if certain attributes like education or age are disproportionately represented, influencing sales patterns. Furthermore, biases could arise if there are inconsistencies in data collection methods across different regions or stores. Addressing these potential biases is crucial to ensure the dataset accurately reflects diverse market dynamics and supports unbiased analysis and decision-making in the car seat industry.





```{r, echo=TRUE}
ggplot(dataset, aes(x = Sales)) +geom_histogram(fill = "lightgreen", color = "blue", bins = 30) +
  labs(title = "Distribution of sales",x = "Miles Per Gallon (Sales)",y = "Frequency")
```

```{r, echo=TRUE}
# checking if missing values are present
colSums(is.na(dataset))
```

__Identify and define prediction problem(s).__



In the context of the "Carseats" dataset, diverse prediction problems can be formulated:
__Sales Prediction:__ By making "Sales" the target variable, and utilizing attributes like competitor prices, advertising budgets, population size, pricing strategies, shelving location quality, demographics, and geography as predictors. This prediction task aims to forecast car seat unit sales for new locations or marketing scenarios.

__Predicting Other Variables with Sales:__ Here, the focus shifts to predicting attributes other than "Sales," using "Sales" as a predictor. For instance, predicting competitor prices or advertising budgets based on car seat sales data. This provides insights into the influence of sales on various marketing and pricing factors.

__Classification Challenge:__ Converting "Sales" into categorical variables like "high sales" and "low sales," and then employing attributes such as competitor prices, advertising budgets, demographics, and location to classify sales categories. This segmentation aids in tailoring marketing strategies for different sales performance segments.
These prediction tasks offer distinct insights into the dynamics driving car seat sales, empowering businesses to optimize their strategies and maximize sales performance.

```{r, echo=TRUE}
# Creating two datasets with only sales and all except sales

Sales_data = dataset$Sales
data_remaining = dataset[, -1]
```

```{r, echo=TRUE}
# classification problem
# converting Sales into a categorical variable
Sales_categorial = ifelse(Sales_data > median(Sales_data), "high_Sales", "low_Sales")
Sales_categorial = as.factor(Sales_categorial)
table(Sales_categorial)
```

__Discuss how to split the data into training and test sets among other plans for the use of data.__

To split the "Carseats" dataset into training and test sets, the `createDataPartition` function from the `caret` package in R can be utilized. This function ensures a random yet balanced partitioning, maintaining the distribution of classes if relevant. Following the split, regression or classification models can be developed using predictors like competitor prices and demographics. These models are then evaluated on the test set using appropriate metrics to select the best-performing one for deployment.

```{r, echo=TRUE}
set.seed(123)

# Spliting the dataset into training (70%) and test (30%) sets
train_index = createDataPartition(Sales_data, p = 0.7, list = FALSE)
train_data = Carseats[train_index, ]
test_data = Carseats[-train_index, ]
dim(train_data)
dim(test_data)
```



## Statistical learning strategies and methods [35 points]

* Perform exploratory data analysis using the training set.
* Describe the statistical learning approaches and other strategies for feature engineering (transformation, selection, etc.).
* Based on the conditions assumed by the statistical learning methods, discuss their applicability to the prediction problem.

__Perform exploratory data analysis using the training set.__


```{r}
summary(train_data)
boxplot(dataset)
```

```{r}
plot1 = ggplot(train_data, aes(x = CompPrice, y = Sales)) +
  geom_point() +labs(title = "Sales vs. CompPrice", x = "CompPrice",y = "Sales")

plot2 = ggplot(train_data, aes(x = Income, y = Sales)) +
  geom_point() +labs(title = "Sales vs. Income", x = "Income",y = "Sales")

plot3 = ggplot(train_data, aes(x = Population, y = Sales)) +
  geom_point() +labs(title = "Sales vs. Population", x = "Population",y = "Sales")

plot4 = ggplot(train_data, aes(x = Price, y = Sales)) +
  geom_point() +labs(title = "Sales vs. Price", x = "Price",y = "Sales")

gridExtra::grid.arrange(plot1, plot2, plot3, plot4)
```


__Sales vs. CompPrice:__ This graphic shows that sales and CompPrice, or the price of the competition, are negatively correlated. Sales often decline when CompPrice rises, indicating that lower competition pricing result in reduced sales for the business.

__Sales vs. Income:__ This graph demonstrates that sales and income have a positive relationship. Sales often tend to rise in tandem with income, suggesting that greater income levels among the populace are linked to increased sales.

__Sales vs. Population:__ This graph shows that there is a marginally positive link between the two variables. Although there is a general tendency toward greater sales in more populous locations, the association is not very strong, and the data points show a great deal of dispersion.

__Sales vs. Price:__ Plotting sales against price shows that there is a negative relationship between the two (price being charged by the firm). Sales usually decline when prices rise, which is to be anticipated as higher prices usually result in less demand.

All things considered, these scatter plots shed light on the connections between sales and other variables including rival prices, income distribution, population size, and the company's own pricing. Businesses may use these data to better understand market dynamics and to guide their pricing, marketing, and sales strategies.

__Describe the statistical learning approaches and other strategies for feature engineering (transformation, selection, etc.).__

```{r}
train_data$logtran_CompPrice = log(train_data$CompPrice)
train_data$logtran_Income = log(train_data$Income)
train_data$logtran_Population = log(train_data$Population)
train_data$logtran_Price = log(train_data$Price)

```

```{r}
plot5 = ggplot(train_data, aes(x = logtran_CompPrice, y = Sales)) +geom_point() +
  labs(title = "Sales vs.  CompPrice Log Tran'tion",x = "log CompPrice",y = "Sales")

plot6 = ggplot(train_data, aes(x = logtran_Price, y = Sales)) +geom_point() +
  labs(title = "Sales vs.  price Log transformation",x = "log price",y = "Sales")

plot7 = ggplot(train_data, aes(x = logtran_Income, y = Sales)) +geom_point() +
  labs(title = "Sales vs.  Income Log Transformation",x = "log Income",y = "Sales")

plot8 = ggplot(train_data, aes(x = logtran_Population, y = Sales)) +geom_point() +
  labs(title = "Sales vs.  Population Log Transformation",x = "log Population",y = "Sales")

gridExtra::grid.arrange(plot5, plot6, plot7, plot8)
```

__Sales vs. CompPrice Log Transformation:__ This graph shows that sales and the log of CompPrice (the price of the competition) are positively correlated. Sales often rise in tandem with the log of CompPrice, indicating a positive correlation between the company's sales and increased competition pricing.


__Sales vs. pricing Log Transformation:__ This figure shows that the log of the company's pricing and sales have a negative relationship. Sales tend to decline as the price log grows, which is consistent with the predicted trend of reduced demand at higher prices.


__Sales vs. Income Log Transformation:__ This graph demonstrates that the log of income and sales have a positive link. Sales often tend to rise in tandem with an increase in the log of income, suggesting that greater income levels among the population are linked to increased sales.


__Sales vs. Population Log Transformation:__ The relationship between sales and the population log is rather positive, as seen in this graphic. After the log transformation, there is a general tendency of greater sales in places with bigger populations; however, the link is not particularly strong, and the data points are highly scattered.


By applying log transformations to the variables, one may lessen the influence of extreme values or skewness in the data and enhance the capture of non-linear correlations. Compared to the original untransformed data, these scatter plots may offer more insights into the underlying patterns between sales and the various components by showing the relationships following the log transformations.
```{r echo=TRUE}
train_data$CompPrice_Population_interaction = train_data$CompPrice * train_data$Population
train_data$CompPrice_Income_interaction = train_data$CompPrice * train_data$Income
train_data$CompPrice_Price_interaction = train_data$CompPrice * train_data$Price
train_data$Income_Population_interaction = train_data$Income * train_data$Population
train_data$Income_Price_interaction = train_data$Income * train_data$Price
train_data$population_Price_interaction = train_data$Population * train_data$Price
```


```{r}
# LASSO regression: fit the model
lasso_regression_model = train(Sales ~ logtran_CompPrice + logtran_Price + logtran_Population + logtran_Income + CompPrice_Population_interaction + CompPrice_Income_interaction + CompPrice_Price_interaction + Income_Population_interaction +  Income_Price_interaction + population_Price_interaction , data = train_data, method = "glmnet", tuneLength = 10)
```

```{r}
# Training linear regression model with selected features
lasso_model_traindata = train_data[, c("Sales", "logtran_CompPrice", "logtran_Price", "logtran_Population", "logtran_Income", "CompPrice_Population_interaction", "CompPrice_Income_interaction", "CompPrice_Price_interaction","Income_Population_interaction", "Income_Price_interaction", "population_Price_interaction")]
```

```{r}
# Scatter plot of Sales vs. log_horsepower
plot9 = ggplot(lasso_model_traindata, aes(x = logtran_CompPrice, y = Sales)) +
  geom_point() +
  labs(title = "Sales vs. CompPrice Log Transformation (LASSO Selected Features)",
       x = "CompPrice Log Transformation",
       y = "Sales")

plot10 = ggplot(lasso_model_traindata, aes(x = logtran_Price, y = Sales)) +
  geom_point() +
  labs(title = "Sales vs. price Log Transformation (LASSO Selected Features)",
       x = "price Log Transformation",
       y = "Sales")

plot11 = ggplot(lasso_model_traindata, aes(x = logtran_Population, y = Sales)) +
  geom_point() +
  labs(title = "Sales vs. Population Log Transformation (LASSO Selected Features)",
       x = "Population Log Transformation",
       y = "Sales")

plot12 = ggplot(lasso_model_traindata, aes(x = logtran_Income, y = Sales)) +
  geom_point() +
  labs(title = "Sales vs. Income Log Transformation (LASSO Selected Features)",
       x = "Income Log Transformation",
       y = "Sales")
gridExtra::grid.arrange(plot9, plot10, plot11, plot12)

```

Sales vs. CompPrice Log Transformation (LASSO): The relationship between sales and the competitive pricing that has been log-transformed is displayed in this plot. The dispersed data points imply a tenuous or nonlinear connection.

Sales vs. pricing Log Transformation (LASSO): the relationship between price log transformation and sales. After LASSO feature selection, the points seem dispersed and lack a distinct linear trend, indicating a tenuous or nuanced link between sales and log-transformed pricing.


Sales vs. Population Log Transformation (LASSO): This scatter plot shows how sales and the population that has been log-transformed are related. A clear pattern formed by the data points hints to a possible positive connection between the two variables.

Sales vs. Income Log Transformation (LASSO): The relationship between sales and the log transformation of income is seen in the graph. In this case, there appears to be a positive association, with sales rising in general in tandem with rises in log-transformed income. After LASSO feature selection, the data points are fairly dispersed and show a moderate to weak linear connection.


```{r}
lm_model = lm(Sales ~ logtran_CompPrice + logtran_Price + logtran_Population + logtran_Income + CompPrice_Population_interaction + CompPrice_Income_interaction + CompPrice_Price_interaction + Income_Population_interaction +  Income_Price_interaction + population_Price_interaction , data = train_data)

summary(lm_model)
```
```{r}
head(lasso_regression_model$results)
```

__Based on the conditions assumed by the statistical learning methods, discuss their applicability to the prediction problem. __

## Lasso Regression


__Assumptions:__

The response variable (sales) and the predictors are assumed to have a linear relationship by the LASSO regression. This assumption is illustrated by the scatter plots, which guarantee a roughly linear connection between the log-transformed predictors and sales.The model's residuals need to have a normal distribution. It is assumed that the residuals of the LASSO model satisfy this condition, even if it isn't checked explicitly in the code.The observations are assumed to be independent of each other. This assumption is essential for reliable coefficient estimates and valid hypothesis testing.At every level of the predictors, the residuals' variance should remain constant. By guaranteeing a constant distribution of points around the regression line, the scatter plots aid in the evaluation of this assumption.

__Applicability:__

When working with high-dimensional datasets that may include collinear predictors, LASSO regression is appropriate. Effective variable selection is achieved using LASSO, which automatically eliminates less significant predictors by penalizing the absolute magnitude of the coefficients.The LASSO model's chosen characteristics provide light on the predictors that have the most influence on sales prediction. This helps guiding company choices and helping to understand the fundamental determinants of sales success.We may capture any nonlinear interactions between predictors and sales by inserting interaction terms in the model, which increases the model's predictive ability and flexibility.Sales scatter plots versus log-transformed variables provide a visual representation of the correlations, which helps with model understanding and validation.


## Linear Regression

__Assumptions:__

The variables (log-transformed CompPrice, Price, Population, Income, and their interactions) and the response variable (Sales) are assumed to have a linear relationship by the linear regression model. Diagnostic plots should be used to verify the assumption that changes in the predictors correspond to changes in the response variable.The residuals, or the variations between the values that were seen and those that were predicted, are assumed to have a normal distribution. This assumption may be evaluated using statistical tests or residual plots and is crucial for precise model parameter estimates and hypothesis testing.It is presumed that the observations are unrelated to one another. This indicates that one observation's value is independent of another observation's value. Valid hypothesis testing and trustworthy coefficient estimates depend on independence.At every level of the predictors, the residuals' variance should remain constant. Stated differently, the residual distribution should exhibit consistency throughout the whole range of predictor values. Residual plots can be used to evaluate this assumption.


__Applicability:__

Predictors and the response variable have a linear connection that makes it possible to forecast continuous outcomes like sales.It makes interpretable coefficients available, which makes it easier to comprehend how predictors affect sales.
When the presumptions are satisfied, it may be utilized for prediction, yielding sales predictions based on predictor values.It enables comparison with other models, such as the best-performing model for forecasting sales in the "Carseats" dataset, such as LASSO regression for variable selection and Ridge regression for multicollinearity handling.





## Predictive analysis and results [35 points]

* Apply and document the statistical learning procedure for the predictive analysis.
* Estimate the performance of the statistical learning approaches on test data, using resampling methods or other measures.
* Evaluate the performance on the test data.
* Discuss the results.



__Apply and document the statistical learning procedure for the predictive analysis.__

```{r}
## Linear Regression Model training
model = lm(Sales ~ CompPrice + Income + Population+ Advertising + ShelveLoc + Age + Education + Urban + US, Price, data = train_data)
summary(model)

lm_predictions = predict(model, newdata = test_data)
```


__Estimate the performance of the statistical learning approaches on test data, using resampling methods or other measures.__

```{r}
lm_root_mean_sqrt_error = sqrt(mean((lm_predictions - test_data$Sales)^2))
cat("RSME:", lm_root_mean_sqrt_error, "\n")

lm_mean_abslt_error = mean(abs(lm_predictions - test_data$Sales))
cat("Mean Absolute Error:", lm_mean_abslt_error, "\n")

lm_R_squared = summary(model)$r.squared
cat("R-squared:", lm_R_squared, "\n")


lm_adjusted_r_squared = 1 - (1 - lm_R_squared) * (nrow(train_data) - 1)/(nrow(train_data) - ncol(train_data) - 1)
cat("adjusted R-squared:", lm_adjusted_r_squared, "\n")
lm_mse = mean((lm_predictions - test_data$Sales)^2)
cat("Mean Squared Error (MSE):", lm_mse, "\n")

accuracy_of_lmmodel = cor(lm_predictions, test_data$Sales)
cat("Accuracy of the Linear Regression model:", accuracy_of_lmmodel, "\n")

f1 = 2 * (accuracy_of_lmmodel * (1 - accuracy_of_lmmodel)) / (accuracy_of_lmmodel + (1 - accuracy_of_lmmodel))
lm_confusion_matrix = table(lm_predictions, test_data$Sales)
```

__cross validation for linear regression model__

```{r}
control_lm = trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_fit = train(Sales ~ CompPrice + Income + Population+ Advertising + ShelveLoc + Age + 
                 Education + Urban + US + Price, data = train_data,method = "lm",
               trControl = control_lm)
lm_predictions1 = predict(lm_fit, newdata = test_data)

lm_root_mean_sqrt_error1 = sqrt(mean((lm_predictions1 - test_data$Sales)^2))
cat(" RMSE:", lm_root_mean_sqrt_error1, "\n")

lm_mean_abslt_error1 = mean(abs(lm_predictions1 - test_data$Sales))
cat(" MAE:", lm_mean_abslt_error1, "\n")

lm_mse1 = mean((lm_predictions1 - test_data$Sales)^2)
cat(" MSE:", lm_mse1, "\n")

accuracy_of_lmmodel1 = cor(lm_predictions1, test_data$Sales)
cat(" Accuracy:", accuracy_of_lmmodel1, "\n")

lm_adjusted_r_squared1 = cor(lm_predictions1, test_data$Sales)^2
cat(" R-squared:", lm_adjusted_r_squared1, "\n")

lm_confusion_matrix1 = table(lm_predictions1, test_data$Sales)


```



```{r}
## Random forest model

library(randomForest)
randomforest_model = randomForest(Sales ~ CompPrice + Income + Population+ Advertising + 
    ShelveLoc + Age + Education + Urban + US + Price, data = train_data, importance =TRUE)
randomforest_predictions = predict(randomforest_model, newdata = test_data)
```


```{r}
rf_root_mean_sqrt_error = sqrt(mean((randomforest_predictions - test_data$Sales)^2))
cat("Root Mean Squared Error (RMSE):", rf_root_mean_sqrt_error, "\n")

rf_mean_abslt_error = mean(abs(randomforest_predictions - test_data$Sales))
cat("Mean Absolute Error (MAE):", rf_mean_abslt_error, "\n")

rf_R_squared = cor(randomforest_predictions , test_data$Sales)^2
cat("R-squared:", rf_R_squared, "\n")

rf_mse = mean((randomforest_predictions - test_data$Sales)^2)
cat("Mean Squared Error (MSE):", rf_mse, "\n")

accuracy_of_randomforest = cor(randomforest_predictions, test_data$Sales)
cat("Accuracy:", accuracy_of_randomforest, "\n")

rf_f1 = 2 * (accuracy_of_randomforest * (1 - accuracy_of_randomforest)) / (accuracy_of_randomforest + (1 - accuracy_of_randomforest))

rf_confusion_matrix = table(randomforest_predictions, test_data$Sales)
rf_importance = randomforest_model$importance



```


__cross - validation Random Forest model__


```{r}
rf_control = train(Sales ~ CompPrice + Income + Population+ Advertising + ShelveLoc + Age +
Education + Urban + US +  Price, data = train_data,method = "rf",
  trControl = trainControl(method = "cv", number = 10))

rf_control_predictions = predict(rf_control, newdata = test_data)

rf_root_mean_sqrt_error1 = sqrt(mean((rf_control_predictions - test_data$Sales)^2))
cat(" RMSE:", rf_root_mean_sqrt_error1, "\n")

rf_mean_abslt_error1 = mean(abs(rf_control_predictions - test_data$Sales))
cat(" MAE:", rf_mean_abslt_error1, "\n")

rf_mse1 = mean((rf_control_predictions - test_data$Sales)^2)
cat(" MSE:", rf_mse1, "\n")

rf_R_squared1 = cor(rf_control_predictions, test_data$Sales)^2
cat(" R-squared:", rf_R_squared1, "\n")

accuracy_of_randomforest1 = cor(rf_control_predictions, test_data$Sales)
cat(" Accuracy:", accuracy_of_randomforest1, "\n")

rf_confusion_matrix1 = table(rf_control_predictions, test_data$Sales)


```


__Decision Tree__

```{r}

library(rpart)
library(rpart.plot)
decision_tree_model = rpart(Sales ~ CompPrice + Income + Population+ Advertising + 
ShelveLoc + Age + Education + Urban + US +  Price, data = train_data)
rpart.plot(decision_tree_model, extra = 1, type = 1, under = TRUE, main = "Decision Tree")
decision_tree_predictions = predict(decision_tree_model, newdata = train_data)


```






```{r}
dt_root_mean_sqrt_error = sqrt(mean((predict(decision_tree_model, newdata = test_data) - test_data$Sales)^2))
cat("Root Mean Squared Error (RMSE):", dt_root_mean_sqrt_error, "\n")

dt_mean_abslt_error = mean(abs(predict(decision_tree_model, newdata = test_data) - test_data$Sales))
cat("Mean Absolute Error (MAE):", dt_mean_abslt_error, "\n")


mean_target = mean(train_data$Sales)
TSS = sum((train_data$Sales - mean_target)^2)
RSS = sum((train_data$Sales - decision_tree_predictions)^2)
dt_R_squared = 1 - (RSS / TSS)
cat("R - Squared", dt_R_squared, "\n")



dt_mean_sqrt_error = mean((predict(decision_tree_model, newdata = test_data) - test_data$Sales)^2)
cat("Mean Squared Error (MSE):", dt_mean_sqrt_error, "\n")

accuracy_of_decicion_tree = cor(predict(decision_tree_model, newdata = test_data), test_data$Sales)
cat("Accuracy:", accuracy_of_decicion_tree, "\n")

decision_tree_f1 = 2 * (accuracy_of_decicion_tree * (1 - accuracy_of_decicion_tree)) / (accuracy_of_decicion_tree + (1 - accuracy_of_decicion_tree))


dt_confusion_matrix = table(predict(decision_tree_model, newdata = test_data), test_data$Sales)

```


```{r}
dt_control = trainControl(method = "cv", number = 10)

dt_fit = train(Sales ~ CompPrice + Income + Population+ Advertising + ShelveLoc + Age + Education + Urban + US +  Price, data = train_data,
                method = "rpart",
                trControl = dt_control)

dt_predictions = predict(dt_fit, newdata = test_data)

dt_root_mean_sqrt_error1 = sqrt(mean((dt_predictions - test_data$Sales)^2))
cat(" RMSE:", dt_root_mean_sqrt_error1, "\n")

dt_mean_abslt_error1 = mean(abs(dt_predictions - test_data$Sales))
cat(" MAE:", dt_mean_abslt_error1, "\n")

dt_mean_sqrt_error1 = mean((dt_predictions - test_data$Sales)^2)
cat(" MSE:", dt_mean_sqrt_error1, "\n")

dt_R_squared1 = cor(dt_predictions, test_data$Sales)^2
cat(" R-squared:", dt_R_squared1, "\n")

accuracy_of_decicion_tree = cor(dt_predictions, test_data$Sales)
cat(" Accuracy:", accuracy_of_decicion_tree, "\n")

dt_confusion_matrix = table(dt_predictions, test_data$Sales)


```


## Comparing the above three models with the results:

The Random Forest model is the most promising among the Linear Regression, Random Forest, and Decision Tree models for predicting the target variable. In comparison to the other models, it exhibits the lowest Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE), showing higher accuracy in predicting the target variable. Furthermore, the Random Forest model has the greatest R-squared value, indicating greater variation in the data and stronger explanatory ability. In addition to achieving the best accuracy, this model suggests a stronger capacity for accurate instance classification. But when compared to Linear Regression, the F1 scores for both the Random Forest and Decision Tree models are lower, suggesting a trade-off between precision and predictive accuracy in classification tasks. In terms of performance metrics, the Random Forest model outperforms the Decision Tree model, whereas the Linear Regression approach lags behind in terms of prediction accuracy. Because of its overall higher performance in predicting the target variable in this particular scenario, the Random Forest model is thus the recommended option.











## Evaluate the performance on the test data.

```{r}
plot13 = ggplot(test_data, aes(x = Sales, y = lm_predictions)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(title = "Actual Sales vs. Predicted Sales per Gallon by Linear Regression",
       x = "Actual Sales per Gallon",
       y = "Predicted Sales per Gallon")

plot14 = ggplot(test_data, aes(x = Sales, y = randomforest_predictions)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(title = "Actual Sales vs. Predicted Sales per Gallon by Random Forest",
       x = "Actual Sales per Gallon",
       y = "Predicted Sales per Gallon")


plot15 = ggplot(test_data, aes(x = Sales, y = dt_predictions)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(title = "Actual Sales vs. Predicted Sales per Gallon by Decision Tree",
       x = "Actual Sales per Gallon",
       y = "Predicted Sales per Gallon")

plot16 = ggplot(test_data, aes(x = Sales, y = lm_predictions1)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(title = "Actual Sales vs. Predicted Sales per Gallon by Linear Regression cross validation",
       x = "Actual Sales per Gallon",
       y = "Predicted Sales per Gallon ")


plot17 = ggplot(test_data, aes(x = Sales, y = rf_control_predictions)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(title = "Actual Sales vs. Predicted Sales per Gallon by Random Forest cross validation",
       x = "Actual Sales per Gallon",
       y = "Predicted Sales per Gallon")


plot18 = ggplot(test_data, aes(x = Sales, y = dt_predictions)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(title = "Actual Sales vs. Predicted Sales per Gallon by Decision Tree cross validation",
       x = "Actual Sales per Gallon",
       y = "Predicted Sales per Gallon")


gridExtra::grid.arrange(plot13 , plot14 , plot15,plot16, plot17, plot18 )
```
__(plot13)__ The graph shows a scatter plot of real sales per gallon vs sales per gallon that were estimated using linear regression. A red linear regression line, which shows the expected sales based on the actual sales numbers, is surrounded by the data points. The regression line has an upward slope, showing that when real sales grow, the anticipated sales also tend to increase. The graph displays a positive association between actual and predicted sales. The data points do, however, occasionally deviate from the regression line, indicating that the linear model does not adequately describe the data.


__(plot14)__ Data points show a positive correlation in the scatter plot that compares real sales per gallon to expected sales per gallon using a random forest model. The red curve shows a rising trend, showing that when actual sales rise, forecasted sales also tend to rise. The curved curvature of the curve indicates that the random forest model captures nonlinear patterns more effectively than the linear regression model. Overall, the random forest model closely matches the data, even though some data points stray from the curve. This suggests that it performs better predictively than linear regression for this dataset.


__(plot15)__ The scatter plot illustrates actual sales per gallon versus predicted sales per gallon using a decision tree model, showcasing a piecewise linear pattern with the red line representing predictions based on actual sales values. Unlike the random forest model, this approach partitions the feature space into distinct regions, resulting in multiple line segments with varying slopes. While it captures some nonlinearity, the decision tree model appears less flexible, evident from its rigid, segmented nature and deviations of data points from the predicted line. The choice between models, including linear regression and random forest, depends on considerations like interpretability, computational efficiency, and overall predictive performance tailored to the dataset's characteristics.










__Discuss the results.__

The contrast between the Random Forest, Decision Tree, and Linear Regression models and the corresponding scatter plots shows different prediction capacities and performance traits.
When it comes to forecasting the target variable, the Random Forest model outperforms the Linear Regression and Decision Tree models, showing the lowest RMSE and MAE, greatest R-squared, and accuracy. For this dataset, its greater explanatory power and accuracy make it the best option. The Random Forest model performs well overall, with lower F1 scores than Linear Regression but superior prediction accuracy and instance classification.


In comparison to the other models, the Linear Regression model performs rather well, showing a decent R-squared value but larger errors. The scatter plot (plot13) shows a good correlation between actual and expected sales, but sporadic departures from the regression line point to potential issues with capturing the complexity of the data.

The Random Forest model, on the other hand, performs better than the others in terms of prediction accuracy, as seen by its greater R-squared value and smaller errors. The scatter plot (plot14) shows a strong link between actual and expected sales, and the curved pattern indicates how well the model captures nonlinear interactions.

In terms of performance measures, the Decision Tree model is situated between Random Forest and Linear Regression. Scatter plot (plot15) illustrates a segmented pattern, although capturing some nonlinearity; this suggests that it is less flexible than Random Forest.

All things considered, the Random Forest model is the best option for forecasting sales in this dataset when taking into account scatter plots and performance measures. It strikes a compromise between interpretability, accuracy, and the capacity to identify nonlinear correlations.








## Conclusion [15 points]

__Discuss the scope and generalization of the predictive analysis.__

The "Carseats" dataset analysis using sales as the goal variable entails projecting unit sales based on variables such as advertising expenditures, price, demographic information, and geographical specifics. The representativeness of the dataset, precise modeling of these characteristics, and awareness of outside variables influencing sales in various situations are all necessary for generalizability.



__Scope and Generalizability__

The "Carseats" dataset's predictive analysis aims to anticipate unit sales based on many parameters, including price, advertising budgets, demographics, and geography features. The representativeness of the dataset, precise modeling of these characteristics, and taking into account extraneous variables like the state of the economy are all necessary for generalizability. Generalizability is improved by rigorous comprehension of the dataset, cautious model selection, validation processes, and robustness tests. As long as the fundamental linkages hold true and outside influences are sufficiently taken into consideration in subsequent scenarios or populations, the analysis's conclusions can be extended to comparable situations.



## Discuss potential limitations and possibilities for improvement.


When using predictive models such as Random Forest, Decision Tree, and Linear Regression to anticipate sales using the "Carseats" dataset, a number of drawbacks and opportunities for development become apparent.

__Limitations:__

Using predictive models for sales forecasting in the "Carseats" dataset, such as Random Forest, Decision Tree, and Linear Regression, presents a few challenges. First, problems with data quality, such incorrect or missing items, might make the models less reliable. Second, models that are too complicated, such as Random Forests, might impede interpretability and make it difficult to derive useful information. Furthermore, it's possible that the dataset contradicts some linear regression assumptions, such as the linear connection between predictors and sales. In addition, Decision Trees can overfit datasets with a large number of predictors, collecting noise rather than true patterns. Finally, unaccountable external variables such as rival activity or market trends might have a big impact on car seat sales, which could affect how accurate the models forecast.



__Possibilities for Improvement:__

Several techniques may be used to improve the prediction modeling for car seat sales in the "Carseats" dataset. First, careful preprocessing and cleaning methods can reduce problems with the quality of the data, guaranteeing more precise model training. Furthermore, feature engineering may be used to change or build new features in order to better capture key patterns, all guided by domain expertise. Furthermore, overfitting in intricate models like Random Forests may be avoided using regularization strategies like Lasso or Ridge Regression. Predictions from several models can be combined using ensemble techniques, such as gradient boosting, to increase overall predictive performance and resilience. Last but not least, incorporating other datasets with pertinent data—such as economic indicators or competition data—can improve the forecasting ability of the models and take into consideration other factors influencing car seat sales.












